{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of twitter data using SparkML\n",
    "\n",
    "Aim: Classify tweets as positive or negative(sentiment) using machine learning in spark. To understand and explore the SparkML Library of PySpark. \n",
    "Dataset: Annotated tweets from “http://help.sentiment140.com/for-students/” provided by Stanford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   senti                                               data\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\",header=None, encoding = \"ISO-8859-1\",\n",
    "                 usecols=[0,5],names=['senti','data'])\n",
    "df['senti'] = df['senti'].map({0: 0, 4: 1})\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: senti, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.senti.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "\n",
    "p1 = r'@[A-Za-z0-9_]+'\n",
    "p2 = r'https?://[^ ]+'\n",
    "pat = r'|'.join((p1, p2))\n",
    "www_p = r'www.[^ ]+'\n",
    "neg_d = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_p = re.compile(r'\\b(' + '|'.join(neg_d.keys()) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets(data):\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stp = re.sub(pat, '', bom_removed)\n",
    "    stp = re.sub(www_p, '', stp)\n",
    "    lower_case = stp.lower()\n",
    "    neg_handled = neg_p.sub(lambda x: neg_d[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Cleaning\n",
      "\n",
      "Done 100000 of 1600000\n",
      "Done 200000 of 1600000\n",
      "Done 300000 of 1600000\n",
      "Done 400000 of 1600000\n",
      "Done 500000 of 1600000\n",
      "Done 600000 of 1600000\n",
      "Done 700000 of 1600000\n",
      "Done 800000 of 1600000\n",
      "Done 900000 of 1600000\n",
      "Done 1000000 of 1600000\n",
      "Done 1100000 of 1600000\n",
      "Done 1200000 of 1600000\n",
      "Done 1300000 of 1600000\n",
      "Done 1400000 of 1600000\n",
      "Done 1500000 of 1600000\n",
      "Done 1600000 of 1600000\n",
      "CPU times: user 11min 21s, sys: 12.8 s, total: 11min 34s\n",
      "Wall time: 13min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Tweets Cleaning\\n\")\n",
    "clean_tweet = []\n",
    "for i in range(0,len(df)):\n",
    "    if( (i+1)%100000 == 0 ):\n",
    "        print(\"Done %d of %d\" % ( i+1, len(df) ))                                                                    \n",
    "    clean_tweet.append(clean_tweets(df['data'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(clean_tweet,columns=['data'])\n",
    "cdf['class'] = df.senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf.to_csv('data/cleaned_tweets.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SparkContext and loading the data as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: SparkContext already exists in this scope\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc = ps.SparkContext('local[*]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "|_c0|                data|class|\n",
      "+---+--------------------+-----+\n",
      "|  0|awww that bummer ...|    0|\n",
      "|  1|is upset that he ...|    0|\n",
      "|  2|dived many times ...|    0|\n",
      "|  3|my whole body fee...|    0|\n",
      "|  4|no it not behavin...|    0|\n",
      "+---+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1596041"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation - splitting dataset into training and testing data for strong evaluation of models\n",
    "(train_data, test_data) = df.randomSplit([0.98, 0.02], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and Logistic Regression\n",
    "TF-IDF to get the features of the tweets and Logistic Regression to train the model and perform classification of positive and negative tweets. The accuracy is also evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :78.934%\n",
      "CPU times: user 91.6 ms, sys: 9.22 ms, total: 101 ms\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_data)\n",
    "predictions = pipelineFit.transform(test_data)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_data.count())\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy :{0:.3%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer, IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.7946\n",
      "CPU times: user 86.9 ms, sys: 14.8 ms, total: 102 ms\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_data)\n",
    "predictions = pipelineFit.transform(test_data)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_data.count())\n",
    "print(\"Accuracy :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#Incase you get java.lang.OutOfMemoryError, use the following command when starting pyspark or \n",
    "# set setting permanently in spark-defaults.conf file \n",
    "#pyspark --driver-memory 5g --executor-memory 7g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.7625\n",
      "CPU times: user 1.83 s, sys: 332 ms, total: 2.16 s\n",
      "Wall time: 6h 41min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\n",
    "word2Vec = Word2Vec(vectorSize=1000, minCount=5, inputCol=\"words\", outputCol=\"features\")\n",
    "label_stringIdx = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, word2Vec, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_data)\n",
    "predictions = pipelineFit.transform(test_data)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_data.count())\n",
    "\n",
    "print(\"Accuracy :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Features and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.8107\n",
      "CPU times: user 249 ms, sys: 54 ms, total: 303 ms\n",
      "Wall time: 14min 34s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "def gen_ngram(inputCol=[\"data\",\"class\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"data\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    crossValidator = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"class\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + crossValidator + idf+ assembler + label_stringIdx + lr)\n",
    "\n",
    "\n",
    "ngram_pipelineFit = gen_ngram().fit(train_data)\n",
    "predictions = ngram_pipelineFit.transform(test_data)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_data.count())\n",
    "\n",
    "print(\"Accuracy :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Among the above models, the one with higher accuracy(NGram) is selected and trained with randomforest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_features(inputCol=[\"data\",\"class\"], n=3):\n",
    "    #create a transformer -tokenizer & ngrams\n",
    "    tokenizer = [Tokenizer(inputCol=\"data\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    crossValidator = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    #create a single column with all the features collated together\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"class\", outputCol = \"label\")]\n",
    "    return Pipeline(stages=tokenizer + ngrams + crossValidator + idf+ assembler + label_stringIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.6998\n",
      "CPU times: user 565 ms, sys: 52.8 ms, total: 618 ms\n",
      "Wall time: 1h 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngram_pipelineFit = ngram_features().fit(train_data)\n",
    "training_df = ngram_pipelineFit.transform(train_data)\n",
    "testing_df = ngram_pipelineFit.transform(test_data)\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = 100, maxDepth = 4, maxBins = 32)\n",
    "rfModel = rf.fit(training_df)\n",
    "predictions = rfModel.transform(testing_df)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(testing_df.count())\n",
    "print(\"Accuracy :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "Among the above models, the one with higher accuracy(NGram) is selected and trained with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.7820\n",
      "CPU times: user 287 ms, sys: 77.5 ms, total: 364 ms\n",
      "Wall time: 17min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngram_pipelineFit = ngram_features().fit(train_data)\n",
    "training_df = ngram_pipelineFit.transform(train_data)\n",
    "testing_df = ngram_pipelineFit.transform(test_data)\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "nbmodel = nb.fit(training_df)\n",
    "predictions = nbmodel.transform(testing_df)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(testing_df.count())\n",
    "print(\"Accuracy :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models=[\"TF-IDF-LR\", \"CV-IDF-LR\", \"Word2Vec-LR\", \"NGram-LR\", \"NGram-RF\", \"NGram-NB\"]\n",
    "Accuracy=[0.7893,0.7946,0.7625,0.8107,0.6998,0.7820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Models':Models,'Accuracy':Accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Models  Accuracy\n",
      "0    TF-IDF-LR    0.7893\n",
      "1    CV-IDF-LR    0.7946\n",
      "2  Word2Vec-LR    0.7625\n",
      "3     NGram-LR    0.8107\n",
      "4     NGram-RF    0.6998\n",
      "5     NGram-NB    0.7820\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e22ed42390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAD8CAYAAAAFdLF9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYXHWd7/H3J2EJWUjIIqMECEojAqETBFHEsRHQ6EU0gIiG0aDAHa4K6kRFA04UvXqxGYSBAcNikMskCJooXsQt1JVFdrIDPoBhCEjUhK3IInR/549zOhwq1d3V1V1dh8Pn9Tz15Cy/c86nKw3f/H5nU0RgZmZWNEOaHcDMzKwRXODMzKyQXODMzKyQXODMzKyQXODMzKyQXODMzKyQXODMzKyQXODMzKyQXODMzKyQtml2gNeyMWPGxJ577tnsGN164YUXGDFiRLNjVOVs9ctzPmerX57zDXS2e++9928RMaG3di5wTbTzzjtzzz33NDtGt0qlEm1tbc2OUZWz1S/P+ZytfnnON9DZJD1WSzsPUZqZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSH5Prgm6uzcQKmkZsfoVrncTql0WLNjVOVs9ctzPmerX7PztbVF047dHffgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskHJX4CSFpPMy87MkzcnMnyhpmaSVkpZKulzSmEHI1ZZm+2Bm2S8ktaXTJUkPSVoi6QFJpzY6k5mZdS93BQ7YDBwjaXzlCknTgC8A74+IfYEDgNuBnau0HdqAbGuA2T2snxERU4B3Av9H0nYNyGBmZjXIY4F7CZhLUsgqzQZmRcQTABHRERFXRsRDAJJWS/q6pFuBj0g6RdLdaU/vJ5KGp+3mSbpE0s2SHpX0bklXpj2veT1kWwo8K+nIXn6GkcALQEeffnIzMxsweb3R+2JgmaRzK5bvC9zXy7abIuJQAEnjIuKydPpbwKeBf0/b7QS8BzgauIGk13UycLekKRGxpJv9fyv9/KbKumskbQZagM9HxFYFLh26PBVgwoTxlMvtvfw4zdPRMTG3+ZytfnnO52z1a3a+UqnU7bpyudzj+kbJZYGLiOck/Qg4HdhYrY2kycDVwCjgaxFxbbrq2kyz/dLCNoakV/WrzLobIiIkLQfWRsTydL8rgUlA1QIXEbdIQtK7qqyeERH3SJoA3C7ppoh4rGL7uSQ9VFpado2RI2d1/0U0WbncTl7zOVv98pzP2erX7Hw9PcmkWW8bz+MQZZfvk/S4RmSWrSQ570ZELE/Pd/0S2CHT5oXM9DzgsxExGfgGMCyzbnP6Z2dmumt+G0nT0wtGlkg6sCLbt+nhXFxE/JWkp3lwjz+hmZk1TG4LXESsB35MUuS6fAdolzQxs2wHujcK+LOkbYEZfTz+woiYkn7uqVj3a5IhztZq26bn+qYCj/TlmGZmNnByOUSZcR7w2a6ZiLgxHf77ZXqV5DPACl459Jh1NnAn8BiwnKTgDZRvAz+rWHaNpI3A9sC8iLh3AI9nZmZ9kLsCFxEjM9NrgeEV668Crupm20kV85cAl1RpNzMzvRrYr9q6im1KQCkz/3NAmfm2atuZmVlz5HaI0szMrD9c4MzMrJBc4MzMrJBc4MzMrJByd5HJa8mQIcNz+Zr3LsnNmfnM52z1y3M+Z6tf3vM1g3twZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSC5wZmZWSH6jdxN1dm6gVFKzY3SrXG6nVDqs2TGqcrb65Tmfs/XHzc0OkDvuwZmZWSG5wJmZWSG5wJmZWSG5wJmZWSHltsBJCknnZeZnSZqTmT9R0jJJKyUtlXS5pDGDkKtN0rOS7pf0oKT2zLqZkv4qaUn6+VGj85iZWXW5LXDAZuAYSeMrV0iaBnwBeH9E7AscANwO7Fyl7dAGZLslIqYCU4GjJL0zs+7aiJiSfj7RgGObmVkN8nybwEvAXJJCNrti3WxgVkQ8ARARHcCVXSslrU7n3wtcJGkUcCqwHfAw8E8RsUHSPGAjsDewO3AS8EngHcCdETGzp4ARsVHSEmCX/vygZmY28PLcgwO4GJghaXTF8n2B+3rZdlNEHBoRC4CfRsRBEdEKPAB8OtNuJ+A9JIX0BuD8dP+TJU3p6QCSdgJagN9nFn80M0R5Ui8ZzcysQfLcgyMinkvPY51O0tPaiqTJwNXAKOBrEXFtuuraTLP9JH0LGAOMBH6VWXdDRISk5cDaiFie7nclMAlYUuWw75K0DHgz8N2IeCqz7tqI+Gx3P5OkU0l6k0yYMJ5yub27pk3X0TExt/mcrX55zuds/VGmVCo1O0RV5XJzsuW6wKW+T9Jb+2Fm2UqS8243pwVpiqSLgB0ybV7ITM8DPhwRSyXNBNoy6zanf3Zmprvmt5E0HfjXdNnJ6Z+3RMRRkvYCbpW0MCKqFcKtRMRckqFXWlp2jZEjZ9WyWVOUy+3kNZ+z1S/P+ZytP26mra2t2SGqKpVKTcmW9yFKImI98GNeOaz4HaBd0sTMsh3o3ijgz5K2BWb08fgLMxeN3FOx7o9plq/0ZZ9mZtZ4r4YeHMB5wJZhv4i4UdIE4JfpVZLPACt45dBj1tnAncBjwHKSgjdQLgVmSdpjAPdpZmb9lNsCFxEjM9NrgeEV668Crupm20kV85cAl1RpNzMzvRrYr9q6im1KQCkzv5GXr6L8E8lwqJmZNVnuhyjNzMzq4QJnZmaF5AJnZmaF5AJnZmaFlNuLTF4LhgwZTltbNDtGt5J7V/KZz9nql+d8zla/vN7k3UzuwZmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSG5wJmZWSEpIr9vqC26lpZd47LL1jQ7RrfK5XZGjpzV7BhVOVv98pzP2eqX53yV2fr7ZnRJ90bEgb21cw/OzMwKyQXOzMwKyQXOzMwKyQXOzMwKKbcFTlJIOi8zP0vSnMz8iZKWSVopaamkyyWNGYRcbZJ+UWV5SdJDaZa7JU1pdBYzM+tebgscsBk4RtL4yhWSpgFfAN4fEfsCBwC3AztXaTu00UEzZkREK/AfwPcG8bhmZlYhzwXuJWAuSSGrNBuYFRFPAERER0RcGREPAUhaLenrkm4FPiLplLRXtVTSTyQNT9vNk3SJpJslPSrp3ZKulPSApHn9yP4HYJd+bG9mZv2U5wIHcDEwQ9LoiuX7Avf1su2miDg0IhYAP42Ig9Le1QPApzPtdgLeQ1JIbwDOT/c/uR/DjNOARXVua2ZmA2CbZgfoSUQ8J+lHwOnAxmptJE0GrgZGAV+LiGvTVddmmu0n6VvAGGAk8KvMuhsiIiQtB9ZGxPJ0vyuBScCSPkS+RtIIYCjJsGm1vKcCpwJMmDCecrm9D7sfXB0dE3Obz9nql+d8zla/POerzFYqlQbluLkucKnvk/TWfphZtpKkgNycFqQpki4Cdsi0eSEzPQ/4cEQslTQTaMus25z+2ZmZ7prfRtJ04F/TZSf3knUGsBT4Lknv85jKBhExl2TolZaWXSOvTx6AV9eTEfIkz9kg3/mcrX55zjfQTzKpVd6HKImI9cCPeeWw4neAdkkTM8t2oHujgD9L2pakCPXl+AsjYkr6uaeG9i8CZwFvl/SWvhzLzMwGTk0FTtJHJI1Kp8+S9FNJVYfgGuQ8YMvVlBFxI3Ah8EtJqyTdDnTwyqHHrLOBO4HfAA8OQJ7DJa3JfN6RXRkRG9PM+fznlJnZa0CtQ5RnR8R1kg4F3ge0A5cABzcqWESMzEyvBYZXrL8KuKqbbSdVzF9Ckrey3czM9Gpgv2rrKrYpUb232FbR7rwqbczMbJDUOkTZkf75P4BLIuJnwHaNiWRmZtZ/tRa4JyT9ADgeuFHS9n3Y1szMbNDVWqSOJzm/NS0ingHGAl9qWCozM7N+6vEcnKSxmdlSZtlmoNcrCs3MzJqlt4tM7gUCUJV1AbxxwBO9hgwZMnzQ7gepR6lUym0+Z6tfnvMVKduLL77ImjVr2LRpUwNTvWz06E0MG7ZqUI7VV5XZHnjggZq2GzZsGBMnTmTbbbet67g9FriI2KOuvZqZvcatWbOGUaNGMWnSJKRqfYSB9fzzzzNq1KiGH6ce9WSLCNatW8eaNWvYY4/6SlGt98EpfT3N2en8bpLeVtcRzcxeAzZt2sS4ceMGpbgVkSTGjRvXrx5wrReZ/AfwDuDj6fzzJI+iMjOzbri49U9/v79aC9zBEfEZYBNARDyN74MzM8u9hQsXIokHHxyIhzi9utT6JJMX0xeHBoCkCSQPIzYzsxqUSgPbm6v1gpf58+dz6KGHsmDBAubMmTOgGbp0dHQwdOhgvlu6NrX24C4EFgKvk/Rt4FbgfzcslZmZ9Vu5XOa2227jiiuuYMGCBVuWn3vuuUyePJnW1lbOPPNMAB5++GGOOOIIWltbOeCAA3jkkUcolUocddRRW7b77Gc/y7x58wCYNGkS3/zmNzn00EO57rrruOyyyzjooINobW3l2GOPZcOGDQCsXbuWj3/847S2ttLa2srtt9/O2WefzQUXXLBlv7Nnz+bCCy8c8J+/ph5cRFwj6V7gcJJbBj4cEbVd52lmZk2xaNEipk2bxl577cXYsWO57777WLt2LYsWLeLOO+9k+PDhrF+/HoAZM2Zw5plnMn36dDZt2kRnZyePP/54j/sfNmwYt956KwDr1q3jlFNOAeCss87iiiuu4HOf+xynn34673znO7nhhhvo6OigXC7zhje8gWOOOYYzzjiDzs5OFixYwF133TXgP39fbvT+CzA/uy59lY2ZmeXQ/Pnz+fznPw/ACSecwPz58+ns7OSkk05i+PDk+fVjx47l+eef54knnmD69OlAUrhq8dGPfnTL9IoVKzjrrLN45plnKJfLvO997wNg8eLFXHxxck3i0KFDGT16NKNHj2bcuHHcf//9rF27lqlTpzJu3LgB+7m79OVG792Ap9PpMcB/Ab5Pzswsh9atW8fixYtZsWIFkujo6EASxx577FZXJ0ZUP5+3zTbb0Nn58uUWlZfsjxgxYsv0zJkzWbRoEa2trcybN6/Xt3affPLJzJs3j6eeeopPfepTffzpatPjObiI2CMi3kjyHMoPRsT4iBgHHAX8tCGJzMys366//no+8YlP8Nhjj7F69Woef/xx9thjD8aOHcuVV1655RzZ+vXr2XHHHZk4cSKLFi0CYPPmzWzYsIHdd9+dVatWsXnzZp599ll+97vfdXu8559/nte//vW8+OKLXHPNNVuWH3744Vx++eVAcjHKc889B8D06dO56aabuPvuu7f09gZarReZHJS+ZBSAiPgl8O6GJDIzs36bP3/+liHHLsceeyxPPvkkRx99NAceeCBTpkyhvb0dgKuvvpoLL7yQ/fffn0MOOYSnnnqKXXfdleOPP57999+fGTNmMHXq1G6Pd84553DwwQdz5JFHsvfee29ZfsEFF3DLLbcwefJk3vrWt7Jy5UoAtttuOw477DCOP/74hl2BWettAn+TdBbwf0mGLE8E1jUkkZlZAQ32MzarDRGefvrpW6a7rp7s0tLSwuLFi7fa5txzz+Xcc8/davnq1atfMX/aaadx2mmnbdVu5513ZsGCBVs9qquzs5M77riD6667rqcfo19q7cF9DJhAcqvAIuB16TIzM7M+WbVqFXvuuSeHH344LS0tDTtOrbcJrAfOkLQj0BkR5YYlMjOzQttnn3149NFHG36cWh+2PFnS/cByYKWkeyXt19hoZmZm9at1iPIHwBcjYveI2B34F2Bu42KZmb36dXf5vdWmv99frQVuRETcnDloCRjRfXMzs9e2YcOGsW7dOhe5OnW9D67Wm86rqfUqykfTd8Fdnc6fCPyp7qMaAJ2dGwb8AawDqVxup1Q6rNkxqnK2+uU5X5GySTsxYsQcHn98T2rvS9QvYiekpxu2/2HDdq97202bNtVVqLre6F2vWgvcp4BvkNzcLeD3wEl1H9XMrOAinqZcPmPQjlcutzNy5KyG7X/q1Pp7oqVSqcd76Bql1qsonwZO77WhmZlZTvT2sOWf97Q+Io4e2DhmZmYDo7ce3DuAx0neInAnyfCkmZlZ7vVW4P4BOJLkqSUfB/4fMD8iVjY6mJmZWX/09jaBjoi4KSI+CbwdeBgoSfpcbzuWdL6kz2fmfyXp8sz8eZK+WE9oSXMkzUqnvyfpQUnLJC2UNEbSCEnrJI2u2G6RpOPrOWZmH5MkraiyfJ6kP0laImmppMP7cxwzM+ufXq9dlbS9pGNIHrT8GeBCantVzu3AIek+hgDjgX0z6w8Bbqvh+L09Zvo3wH4RsT/wR+CrEfEC8Gvgw5n9jAYOBX5RQ/Z6fSkipgCfBy5t4HHMzKwXPRY4SVeRFKoDgG9ExEERcU5EPFHDvm8jLXAkhW0F8LyknSRtD7wFWJL2wFZIWi7po+lx2yTdLOk/SR4PhqTZkh6S9FvgzV0HiYhfR8RL6ewdQNdNE/OBEzJ5pgM3RcSGtId3paS7Jd0v6UPpMYZKak+zLKulp9qNPwC71LmtmZkNgN7Owf0T8AKwF3B65i2wAiIiduxuw4h4UtJLknYjKXRd/9N/B/AssIzkxalTgFaSHt7dkn6f7uJtJD2zP0l6K0mxmppmvo/kbeOVPgVcm07fBFwuaVxErEu3//d03WxgcUR8StIY4K60cH6C5C3lUyPiJUlje/l+ujON5K0LW5F0KnAqwIQJ4ymX2+s8RON1dEzMbT5nq1+e8zlb/Rqdr7c3dPekXC73a/t69VjgIqK/t9939eIOAf6NpMAdQlLgbicZMpwfER3AWkn/HzgIeA64KyK6npbyLmBhRGyA6rcvSJoNvARck2b/e9ruOEk/ISmkv06bvxc4uus8HjAM2A04Ari0q0eYvkWhL74n6VyS1wm9vVqDiJhL+hzPlpZdo5E3ZvZXo28c7Q9nq1+e8zlb/Rqdrz/vsyuVSrS1tQ1cmBo1+vkxXefhJpMMUd5B0oPrOv/W020HL1TMd/vtSvokSW9wRrzywW9dw5THAT+LiBe7NgGOjYgp6We3iHggXR4V+z44vXBkiaTe7vv7ErAncBZwVS9tzcysgRpd4G4jKTzr0ysy1wNjSIrcH0ge+fXR9NzXBOAfgbuq7Of3wHRJO0gaBXywa4WkacBXgKO7engZNwMtJBfHzM8s/xXwOaVjrpK6niHza+CfJW2TLh8bEXdmCmGPN74DREQncAEwRNL7emtvZmaN0egCt5zk3NodFcuejYi/kbwhfBmwFFgMfDkinqrcSUTcR3JubQnwE+CWzOqLgFHAb9Je1qWZ7TrT9uNIimSXc4BtgWXpJf/npMsvB/4rXb6U5N6/at4saU3m85GKvAF8C/hyN9ubmVmD1fqw5bqk59Z2rFg2MzMdJMN6X6poUwJKFcu+DXy7yjH27CXDGcAZFcs2Av+zStuXgC+mn+72t5qkOFa6rqLdT0iKq5mZNUHj3+FgZmbWBC5wZmZWSC5wZmZWSA09B2c9GzJkeL/uLWm05N6VfOZztvrlOZ+z1S/v+ZrBPTgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskFzgzMyskv9G7iTo7N1AqqdkxulUut1MqHdbsGFU5W/3ynM/Z6pfHfM1+w7h7cGZmVkgucGZmVkgucGZmVkgucGZmVkgucGZmVki5L3CS/kHSAkmPSFol6UZJIenNFe2+L+nLVbafJ+m4dLok6SFJyyQ9KOkiSWMybTskLcl8JvW0v8yySZI2ptuskvQjSdsO1HdgZmZ9l+sCJ0nAQqAUEW+KiH2ArwEl4IRMuyHAccC1Nex2RkTsD+wPbAZ+llm3MSKmZD6r+xD3kYiYAkwGJgLH92FbMzMbYLkucMBhwIsRcWnXgohYApxBpsAB/wisjojHat1xRPwd+DKwm6TWAcpLRHQAdwG7DNQ+zcys7/J+o/d+wL2VCyNimaROSa0RsZSk2M3v684jokPSUmBvYCmwg6Ql6eo/RcT0vu5T0jDgYJIiXG39qcCpABMmjKdcbu/rIQZNR8fE3OZztvrlOZ+z1S+P+UqlEgDlcnnL9GDKe4HryXzgBEkrgQ8BX69zP9lHiWxMhxnr8aa0OLYA10fEsmqNImIuMBegpWXXGDlyVp2Ha7xyuZ285nO2+uU5n7PVL4/5up5kUiqVaGtrG/Tj532IciXw1m7WzSc5z3UEsCwi/gIg6YfpxR439rZzSUNJzpk90EObWvfXdQ5uT+Dtko7u7fhmZtY4eS9wi4HtJZ3StUDSQZLeHRGPAOuA75IZnoyIk9ILRD7Q047Tqxy/AzzeXW+rL/vLtP8zcCbw1Vram5lZY+S6wEVEANOBI9PbBFYCc4An0ybzSc6fLezDbq+RtAxYAYwgGd7sqx9IWpN+/lBl/SJguKR31bFvMzMbALk/BxcRT9LNJfcRcT5wfi/bz8xMt/XSdmQNeWZ2s2q/TJsABuzKTDMz67tc9+DMzMzq5QJnZmaF5AJnZmaF5AJnZmaFlPuLTIpsyJDhTX+le0+SmzPzmc/Z6pfnfM5Wv7znawb34MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJBc4MzMrJD8Ru8m6uzcQKmkZsfoVrncTql0WLNjVOVs9ctzPmerX57y5eXN4u7BmZlZIbnAmZlZIbnAmZlZIbnAmZlZIb0qCpykcZKWpJ+nJD2RmY/M9BJJk6psP0/Scel0SdJDkpZJelDSRZLGZNp29GV/mWWTJG1Mt1kl6UeSth3o78LMzGrzqriKMiLWAVMAJM0ByhHRns6XI2JKH3c5IyLukbQd8B3gZ8C703Ub69hfl0ciYoqkocBvgOOBa+rcl5mZ9cOrogfXKBHxd+DLwG6SWgdwvx3AXcAuA7VPMzPrmyIUuB0yw4kL+7pxWoyWAnsPxP4AJA0DDgZuqmd7MzPrv1fFEGUv+jOk2CV7t3V/9vcmSUuAFuD6iFi21YGkU4FTASZMGE+53F7noRqvo2NibvM5W/3ynM/Z6penfKVS6RXz5XJ5q2WDoQgFbiuSfghMBZ6MiA/00nYoMBl4YAD213UO7vVASdLREfHzbIOImAvMBWhp2TVGjpxV08/UDOVyO3nN52z1y3M+Z6tfnvJVPsmkVCrR1tY26DkKWeAi4qRa2qVXOX4beLxab6uv+8u0/7OkM4GvAj/vrb2ZmQ28IpyDq8c1kpYBK4ARwIfq2McPJK1JP3+osn4RMFzSu/oT1MzM6vOq68FFxJyK+ZE1bDMzM93WS9s+7a/Cfpk2AQzYlZlmZtY3r9UenJmZFZwLnJmZFZILnJmZFZILnJmZFdKr7iKTIhkyZHhu3nxbTXLvSj7zOVv98pzP2eqX93zN4B6cmZkVkgucmZkVkgucmZkVkgucmZkVkgucmZkVkgucmZkVkgucmZkVkgucmZkVkpKH3lszSHoeeKjZOXowHvhbs0N0w9nql+d8zla/POcb6Gy7R8SE3hr5SSbN9VBEHNjsEN2RdE9e8zlb/fKcz9nql+d8zcrmIUozMyskFzgzMyskF7jmmtvsAL3Icz5nq1+e8zlb/fKcrynZfJGJmZkVkntwZmZWSC5wg0DSNEkPSXpY0plV1m8v6dp0/Z2SJuUo2z9Kuk/SS5KOG6xcfcj3RUmrJC2T9DtJu+co2z9LWi5piaRbJe0zWNlqyZdpd5ykkDRoV7nV8N3NlPTX9LtbIunkvGRL2xyf/t6tlPSfeckm6fzMd/ZHSc8MVrYa8+0m6WZJ96f/zX6goYEiwp8GfoChwCPAG4HtgKXAPhVt/hdwaTp9AnBtjrJNAvYHfgQcl8Pv7jBgeDp9Ws6+ux0z00cDN+Xpu0vbjQJ+D9wBHJiXbMBM4KLB/H3rQ7YW4H5gp3T+dXnJVtH+c8CVOfvu5gKnpdP7AKsbmck9uMZ7G/BwRDwaEX8HFgAfqmjzIeCqdPp64HBJykO2iFgdEcuAzkHIU0++myNiQzp7BzAxR9mey8yOAAbzhHctv3cA5wDnAptymK0Zasl2CnBxRDwNEBF/yVG2rI8B8wclWaKWfAHsmE6PBp5sZCAXuMbbBXg8M78mXVa1TUS8BDwLjMtJtmbqa75PA79saKKX1ZRN0mckPUJSRE4fpGxQQz5JU4FdI+IXg5gLav97PTYdxrpe0q6DE62mbHsBe0m6TdIdkqblKBsA6VD9HsDiQcjVpZZ8c4ATJa0BbiTpZTaMC1zjVeuJVf5LvpY2jdCs49aq5nySTgQOBL7X0ESZQ1ZZtlW2iLg4It4EfAU4q+GpXtZjPklDgPOBfxm0RC+r5bu7AZgUEfsDv+XlEY5GqyXbNiTDlG0kvaTLJY1pcC7o23+vJwDXR0RHA/NUqiXfx4B5ETER+ABwdfq72BAucI23Bsj+63MiW3fLt7SRtA1J1319TrI1U035JB0BzAaOjojNecqWsQD4cEMTvVJv+UYB+wElSauBtwM/H6QLTXr97iJiXebv8jLgrYOQq6ZsaZufRcSLEfEnkufJtuQkW5cTGNzhSagt36eBHwNExB+AYSTPqWyMwToB+Vr9kPxr71GS4YKuE6/7VrT5DK+8yOTHecmWaTuPwb/IpJbvbirJie2WHGZryUx/ELgnT/kq2pcYvItMavnuXp+Zng7ckaNs04Cr0unxJMNy4/KQLW33ZmA16X3OefqdIzmFMDOdfgtJAWxYzkH74V/LH5Ku+B/T/xHPTpd9k6THAcm/Yq4DHgbuAt6CAXDoAAAAmUlEQVSYo2wHkfzL7AVgHbAyZ9/db4G1wJL08/McZbsAWJnmurmnAtOMfBVtB63A1fjdfSf97pam393eOcom4N+AVcBy4IS8ZEvn5wDfHczftT58d/sAt6V/r0uA9zYyj59kYmZmheRzcGZmVkgucGZmVkgucGZmVkgucGZmVkgucGZmVkgucGZmVkgucGZmVkgucGZmVkj/Df4dNvGM9pc/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x=\"Models\", y=\"Accuracy\", kind=\"barh\", color = \"y\",grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Thus, from the above plot it is infered that Ngram with Logistic Regression performs better comparatively with a accuracy score of 81%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
